```{python}
# | echo: false
import pandas as pd
from pathlib import Path
from IPython.display import display
import json
from datetime import datetime


def load_and_summarize_results():
    """Load CSV results and create summary table."""

    # Get results directory relative to doc folder
    results_dir = Path("../results")

    if not results_dir.exists():
        return "Results directory not found. Please check the path."

    summary_data = []

    # Define model mappings in the same order as website navigation
    model_mappings = {
        "irm": {
            "category": "IRM",
            "order": 1,
            "files": {
                "irm_ate_coverage.csv": {"type": "Basic", "effect": "ATE"},
                "irm_atte_coverage.csv": {"type": "Basic", "effect": "ATTE"},
                "irm_gate_coverage.csv": {"type": "Heterogeneous", "effect": "GATE"},
                "irm_cate_coverage.csv": {"type": "Heterogeneous", "effect": "CATE"},
                "apo_coverage.csv": {"type": "APO", "effect": "APO"},
                "pq_effect_coverage.csv": {"type": "Quantile", "effect": "QTE"},
                "lpq_effect_coverage.csv": {"type": "Quantile", "effect": "LQTE"},
                "cvar_effect_coverage.csv": {"type": "Risk Measure", "effect": "CVAR"},
                "iivm_late_coverage.csv": {"type": "IIVM", "effect": "LATE"},
                # Add sensitivity analysis files
                "irm_ate_sensitivity_coverage.csv": {"type": "Sensitivity", "effect": "ATE"},
                "irm_atte_sensitivity_coverage.csv": {"type": "Sensitivity", "effect": "ATTE"},
            },
        },
        "plm": {
            "category": "PLM",
            "order": 2,
            "files": {
                "plr_ate_coverage.csv": {"type": "Basic PLR", "effect": "ATE"},
                "plr_gate_coverage.csv": {"type": "Heterogeneous", "effect": "GATE"},
                "plr_cate_coverage.csv": {"type": "Heterogeneous", "effect": "CATE"},
                "pliv_late_coverage.csv": {"type": "PLIV", "effect": "LATE"},
                # Add sensitivity analysis files
                "plr_ate_sensitivity_coverage.csv": {"type": "Sensitivity", "effect": "ATE"},
            },
        },
        "did": {
            "category": "DID",
            "order": 3,
            "files": {
                "did_multi_group.csv": {"type": "Multi-Period", "effect": "Group Effects"},
                "did_multi_eventstudy.csv": {"type": "Multi-Period", "effect": "Event Study"},
                "did_multi_time.csv": {"type": "Multi-Period", "effect": "Time Effects"},
                "did_multi_detailed.csv": {"type": "Multi-Period", "effect": "ATTE Detailed"},
                "did_pa_atte_coverage.csv": {"type": "Panel Data", "effect": "ATTE"},
                "did_cs_atte_coverage.csv": {"type": "Cross-Section", "effect": "ATTE"},
            },
        },
        "ssm": {
            "category": "SSM",
            "order": 4,
            "files": {
                "ssm_mar_ate_coverage.csv": {"type": "MAR", "effect": "ATE"},
                "ssm_nonig_ate_coverage.csv": {"type": "Nonignorable", "effect": "ATE"},
            },
        },
        "rdd": {
            "category": "RDD",
            "order": 5,
            "files": {
                "rdd_sharp_coverage.csv": {"type": "Sharp Design", "effect": "Local Effect"},
                "rdd_fuzzy_coverage.csv": {"type": "Fuzzy Design", "effect": "Local Effect"},
            },
        },
    }

    # Process each model category
    for model_dir, config in model_mappings.items():
        model_path = results_dir / model_dir
        if not model_path.exists():
            continue

        # Look for specific metadata files for different model types
        def find_metadata_file(filename):
            """Find metadata file for a specific coverage file."""
            # For DID multi-period files, always use the common metadata file
            if filename.startswith("did_multi"):
                metadata_path = model_path / "did_multi_metadata.csv"
                if metadata_path.exists():
                    return metadata_path
            
            # For other files, try standard naming
            base_name = filename.replace("_coverage.csv", "")
            metadata_candidates = [
                f"{base_name}_metadata.csv",
                # For QTE/LQTE/CVAR, check the base model metadata
                "pq_metadata.csv" if "pq_effect" in filename else None,
                "lpq_metadata.csv" if "lpq_effect" in filename else None,
                "cvar_metadata.csv" if "cvar_effect" in filename else None,
            ]
            
            for candidate in metadata_candidates:
                if candidate and (model_path / candidate).exists():
                    return model_path / candidate
            return None

        # Process coverage files for this category and create averaged results
        for filename, file_config in config["files"].items():
            file_path = model_path / filename
            if not file_path.exists():
                continue

            # Find appropriate metadata file
            metadata_file = find_metadata_file(filename)

            # Default metadata values
            timestamp = "N/A"
            doubleml_version = "N/A"

            # Try to read metadata
            if metadata_file:
                try:
                    metadata_df = pd.read_csv(metadata_file)

                    # Get the first row of metadata
                    if not metadata_df.empty:
                        first_row = metadata_df.iloc[0]

                        # Look for timestamp columns
                        for col in ["timestamp", "run_time", "date", "time", "Date"]:
                            if col in metadata_df.columns:
                                timestamp = first_row[col]
                                break

                        # Look for version columns
                        for col in ["doubleml_version", "version", "dml_version", "DoubleML Version"]:
                            if col in metadata_df.columns:
                                doubleml_version = first_row[col]
                                break

                    # Format timestamp if available
                    if timestamp != "N/A" and pd.notna(timestamp):
                        try:
                            timestamp_str = str(timestamp).strip()

                            # Try to parse as datetime and format as readable date
                            try:
                                # First try pandas to_datetime which handles most formats
                                parsed_date = pd.to_datetime(timestamp_str)
                                timestamp = parsed_date.strftime("%b %d, %Y")
                            except:
                                # If that fails, just extract date part if there's time info
                                if " " in timestamp_str:
                                    timestamp = timestamp_str.split(" ")[0]
                        except:
                            # If everything fails, keep original value
                            pass

                except Exception as e:
                    pass

            try:
                df = pd.read_csv(file_path)

                # Filter for 95% confidence level results
                if "level" in df.columns:
                    df_95 = df[df["level"] == 0.95].copy()
                else:
                    df_95 = df.copy()

                if df_95.empty:
                    continue

                # Find coverage column - for sensitivity files, prioritize bound coverage
                coverage_col = None
                if "sensitivity" in filename.lower():
                    # For sensitivity analysis, look for bound coverage columns first
                    for col in ["Coverage (Lower)", "Coverage (Upper)", "Coverage", "coverage"]:
                        if col in df_95.columns:
                            coverage_col = col
                            break
                else:
                    # For regular files, use standard coverage
                    for col in ["Coverage", "coverage", "Coverage Rate"]:
                        if col in df_95.columns:
                            coverage_col = col
                            break

                if coverage_col is None:
                    continue

                # Find sample size column
                sample_size_col = None
                for col in ["repetition", "n_rep", "sample_size", "n"]:
                    if col in df_95.columns:
                        sample_size_col = col
                        break

                # Calculate average coverage rate across all combinations
                avg_coverage = df_95[coverage_col].mean()
                total_sample_size = df_95[sample_size_col].iloc[0] if sample_size_col else "N/A"

                # Determine status based on deviation from 0.95
                deviation = abs(avg_coverage - 0.95)
                if deviation <= 0.05:
                    status = "Good"
                elif deviation <= 0.10:
                    status = "Marginal"
                else:
                    status = "Poor"

                # Add coverage type info for sensitivity analysis
                coverage_info = file_config["effect"]
                if "sensitivity" in filename.lower() and "Coverage (Lower)" in coverage_col:
                    coverage_info += " (Lower Bound)"
                elif "sensitivity" in filename.lower() and "Coverage (Upper)" in coverage_col:
                    coverage_info += " (Upper Bound)"

                summary_data.append(
                    {
                        "Model Category": config["category"],
                        "Category Order": config["order"],
                        "Model Type": file_config["type"],
                        "Effect": coverage_info,
                        "Coverage Rate": avg_coverage,
                        "Repetitions": total_sample_size,
                        "Status": status,
                        "Last Modified": timestamp,
                        "DoubleML Version": doubleml_version,
                        "Deviation": deviation,
                    }
                )

            except Exception as e:
                print(f"Error processing {file_path}: {e}")
                continue

    if not summary_data:
        return "No coverage results found in the CSV files."

    return summary_data


# Generate the summary data
summary_data = load_and_summarize_results()

if isinstance(summary_data, str):
    print(summary_data)
else:
    # Create DataFrame and sort by category order first, then by deviation
    df = pd.DataFrame(summary_data)
    df_sorted = df.sort_values(["Category Order", "Deviation"])

    # Display as a nice table using pandas styling
    display_df = df_sorted[
        [
            "Model Category",
            "Model Type",
            "Effect",
            "Coverage Rate",
            "Status",
            "Repetitions",
            "Last Modified",
            "DoubleML Version",
        ]
    ].copy()

    # Format the coverage rate to 3 decimal places
    display_df["Coverage Rate"] = display_df["Coverage Rate"].apply(lambda x: f"{x:.3f}")

    # Apply improved styling
    def style_table(df):
        # Color function for status
        def color_status(val):
            if val == "Good":
                return "background-color: #d4edda; color: #155724; font-weight: bold; padding: 4px 8px; border-radius: 4px; border: 1px solid #c3e6cb;"
            elif val == "Marginal":
                return "background-color: #fff3cd; color: #856404; font-weight: bold; padding: 4px 8px; border-radius: 4px; border: 1px solid #ffeaa7;"
            else:
                return "background-color: #f8d7da; color: #721c24; font-weight: bold; padding: 4px 8px; border-radius: 4px; border: 1px solid #f5c6cb;"

        # Apply status colors
        styled = df.style.map(color_status, subset=["Status"])

        # Set table-wide properties
        styled = styled.set_table_styles(
            [
                {
                    "selector": "thead th",
                    "props": [
                        ("background-color", "#f8f9fa"),
                        ("color", "#495057"),
                        ("font-weight", "bold"),
                        ("text-align", "center"),
                        ("padding", "12px"),
                        ("border", "1px solid #dee2e6"),
                    ],
                },
                {
                    "selector": "tbody td",
                    "props": [("padding", "10px"), ("border", "1px solid #dee2e6"), ("text-align", "center")],
                },
                {
                    "selector": "table",
                    "props": [
                        ("border-collapse", "collapse"),
                        ("margin", "20px auto"),
                        ("box-shadow", "0 2px 8px rgba(0,0,0,0.1)"),
                        ("border-radius", "8px"),
                        ("overflow", "hidden"),
                    ],
                },
                {"selector": "tbody tr:nth-child(even)", "props": [("background-color", "#f8f9fa")]},
                {"selector": "tbody tr:hover", "props": [("background-color", "#e9ecef")]},
            ]
        )

        return styled

    # Create and display styled table
    styled_df = style_table(display_df)
    display(styled_df)
```

The table above summarizes coverage rates for 95% confidence intervals across all DoubleML models tested in this project. Results are color-coded according to the deviation from the nominal 95% coverage rate:

- **Coverage Rate**: Proportion of confidence intervals containing the true parameter (target: 0.95)
- **Repetitions**: Number of independent simulation runs performed
- **Last Modified**: When the simulation was run
- **DoubleML Version**: Version of DoubleML used for the simulation
- **Status**:
  - **Good** (Green): Coverage within 5% of nominal level (0.90-1.00)
  - **Marginal** (Yellow): Coverage deviates 5-10% from nominal level
  - **Poor** (Red): Coverage deviates more than 10% from nominal level

For detailed methodology and complete results, explore the individual model pages using the navigation menu.
